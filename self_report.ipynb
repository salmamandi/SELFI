{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"self_report.ipynb","provenance":[],"authorship_tag":"ABX9TyM0URIrfa4bpupsihKD1w6w"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Run the code following two steps:\n","\n","\n","1.   List the features returned by the image analysis tool (Rekognition/Vision/Azure) by excuting one of the three code blocks\n","2. Run the final code block to evaluate the framework.  \n","\n"],"metadata":{"id":"-trycaw7-0WG"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"spDGJJWLFpjN"},"outputs":[],"source":["pip install pyDML"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"_YCBgQ5fqbSc","executionInfo":{"status":"ok","timestamp":1644281853939,"user_tz":-330,"elapsed":487,"user":{"displayName":"Salma Mandi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjuIgiKzXUJnsrVeYqDh-c8HgFzaMf1rPA6e6_k8A=s64","userId":"16790298494727486665"}}},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from math import *\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import f1_score\n","from sklearn.preprocessing import StandardScaler\n","from keras.utils import np_utils\n","from keras import optimizers\n","import keras.backend as K\n","from sklearn.ensemble import RandomForestClassifier\n","import seaborn as sns\n","from sklearn.decomposition import KernelPCA\n","from sklearn.metrics import classification_report\n","import os\n","import csv\n","import matplotlib.pyplot as plt\n","import fnmatch\n","import datetime\n","import shutil\n","from statistics import mean\n","from sklearn.feature_selection import f_classif\n","from itertools import combinations\n","import math\n","from numpy import percentile\n","from dml import kda"]},{"cell_type":"markdown","source":["Necessary functions to compute self-report features"],"metadata":{"id":"UCNqSQe25hS2"}},{"cell_type":"code","execution_count":5,"metadata":{"id":"a4jcEzKhm5b4","executionInfo":{"status":"ok","timestamp":1644284062826,"user_tz":-330,"elapsed":413,"user":{"displayName":"Salma Mandi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjuIgiKzXUJnsrVeYqDh-c8HgFzaMf1rPA6e6_k8A=s64","userId":"16790298494727486665"}}},"outputs":[],"source":["def find_transition_matrix(values):\n","\tsrc_labels=[0,1]\n","\ttgt_labels=[0,1]\n","\t\n","\ttransition_vector=[]\n","\t\n","\tfor em_src in src_labels:\n","\t\tsrc=values[values[:,0]==em_src,:]\n","\t\tif src.shape[0] != 0:\n","\t\t\tfor em_tgt in tgt_labels:\n","\t\t\t\ttgt=src[src[:,1]==em_tgt,:]\n","\t\t\t\ttrans_prob=tgt.shape[0]/float(src.shape[0])\t\t\t\n","\t\t\t\ttransition_vector.append(trans_prob)\n","\t\telse:\n","\t\t\ttransition_vector.extend([0,0])\n","\treturn transition_vector\n","\n","def find_average_sequence_length(values):\n","\tseq_length_vector=[]\n","\n","\tseq_lengths_l=np.empty((0))\n","\tseq_lengths_h=np.empty((0))\n","\n","\n","\tcount=1\n","\tfor i in range(1,values.shape[0]):\n","\t\tif values[i,1]==0 and values[i,1]==values[i-1,1]:\t\t\t\t\n","\t\t\t\tcount=count+1\n","\t\t\t\tcontinue\n","\t\telse:\n","\t\t\tif values[i-1,1]==0:\n","\t\t\t\tseq_lengths_l=np.append(seq_lengths_l,count)\n","\t\t\t\tcount=1\n","\tif count>1 or seq_lengths_l.shape[0]==0:\n","\t\tseq_lengths_l=np.append(seq_lengths_l,count)\t\n","\n","\tcount=1\n","\tfor i in range(1,values.shape[0]):\n","\t\tif values[i,1]==values[i-1,1] and values[i,1]==1:\n","\t\t\tcount=count+1\n","\t\t\tcontinue\n","\t\telse:\n","\t\t\tif values[i-1,1]==1:\t\t\t\n","\t\t\t\tseq_lengths_h=np.append(seq_lengths_h,count)\n","\t\t\t\tcount=1\n","\tif count>1 or seq_lengths_h.shape[0]==0:\n","\t\tseq_lengths_h=np.append(seq_lengths_h,count)\n","\n","\t\t\n","\n","\tseq_length_vector.append(int(round(np.percentile(seq_lengths_l,75))))\n","\tseq_length_vector.append(int(round(np.percentile(seq_lengths_h,75))))\n","\t\n","\n","\t#print('seq vector:',np.array(seq_length_vector))\n","\treturn np.array(seq_length_vector)\n","\n","def find_PRE(transition_vector,previous_label):\n","\tif previous_label==0:\n","\t\tlabel_vector=np.array([1,0])\n","\telif previous_label==1:\n","\t\tlabel_vector=np.array([0,1])\n","\n","\t\n","\ttransition_matrix=np.empty((0,2))\n","\ttransition_matrix=np.append(transition_matrix,np.array([transition_vector[0:2]]),axis=0)\n","\ttransition_matrix=np.append(transition_matrix,np.array([transition_vector[2:4]]),axis=0)\n","\n","\n","\tpredicted_val=np.matmul(label_vector,transition_matrix)\n","\tidx=np.argmax(predicted_val)\n","\n","\tpredicted_val_sorted=np.sort(predicted_val)\n","\n","\tr=np.where(predicted_val==predicted_val_sorted[0])\n","\tsec_idx=r[0][0]\n","\n","\t\n","\t\n","\tif idx==0:\n","\t\tpre='low'\n","\telif idx==1:\n","\t\tpre='high'\n","\n","\n","\tif sec_idx==0:\n","\t\tpre_2='low'\n","\telif sec_idx==1:\n","\t\tpre_2='high'\n","\t\n","\n","\ttotal_wt=np.sum(predicted_val)\n","\tif total_wt==0:\n","\t\tpredicted_val_norm=np.array([0,0])\n","\telse:\n","\t\tpredicted_val_norm=np.divide(predicted_val,total_wt)\n","\n","\treturn pre,pre_2,predicted_val_norm\n","\n","def find_val(label):\n","   if label=='low':\n","     val=0\n","   elif label=='high':\n","     val=1\n","   return val  \n","def find_label(val):\n","\tif val==0:\n","\t\tlabel='low'\n","\telif val==1:\n","\t\tlabel='high'\n","\n","\treturn label"]},{"cell_type":"markdown","source":["Return facial features value"],"metadata":{"id":"XEEAyrQk5aCl"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"uBIR-z3Bn9Ua","executionInfo":{"status":"ok","timestamp":1644291700151,"user_tz":-330,"elapsed":480,"user":{"displayName":"Salma Mandi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjuIgiKzXUJnsrVeYqDh-c8HgFzaMf1rPA6e6_k8A=s64","userId":"16790298494727486665"}}},"outputs":[],"source":["def facial_feature(photo,common):\n","  user_no=photo.split('/')[0]\n","  # mention facial feature file\n","  # for Amazon\n","  user_file='Data/FacialData/Amazon/'+user_no+'.csv'\n","  # for vision\n","  #user_file='Data/FacialData/Google/'+user_no+'.csv'\n","  # for Azure\n","  #user_file='Data/FacialData/Microsoft/'+user_no+'.csv'\n","  user_data=pd.read_csv(user_file)\n","  df=user_data[user_data['FileName']==photo]\n","  df=df[common]\n","  #print(df)\n","  feature_list=[]\n","\n","  df=df.to_numpy()\n","  #print(df)\n","  for k in range(df.shape[1]):\n","    feature_list.append(df[0,k])\n","  #print(feature_list)\n","  return feature_list"]},{"cell_type":"markdown","source":["Remove duplicate data samples if any"],"metadata":{"id":"9p5Bk-cZ5TuX"}},{"cell_type":"code","execution_count":7,"metadata":{"id":"9Suil70EoCn2","executionInfo":{"status":"ok","timestamp":1644284072108,"user_tz":-330,"elapsed":445,"user":{"displayName":"Salma Mandi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjuIgiKzXUJnsrVeYqDh-c8HgFzaMf1rPA6e6_k8A=s64","userId":"16790298494727486665"}}},"outputs":[],"source":["def data_stat(train_data):\n","  np.set_printoptions(suppress=True)\n","  uniquerows=np.unique(train_data,axis=0)\n","  print(\"number of unique rows=\",uniquerows.shape)\n","  return uniquerows"]},{"cell_type":"markdown","source":["Build the model and evaluate"],"metadata":{"id":"g6yImOwR5PDF"}},{"cell_type":"code","execution_count":12,"metadata":{"id":"hBInTVV5oI-n","executionInfo":{"status":"ok","timestamp":1644284157977,"user_tz":-330,"elapsed":434,"user":{"displayName":"Salma Mandi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjuIgiKzXUJnsrVeYqDh-c8HgFzaMf1rPA6e6_k8A=s64","userId":"16790298494727486665"}}},"outputs":[],"source":["def Random_Forest_model(train_data,test_data):\n","\n","  \n","  print(\"train size in model function=\",train_data.shape[0],\"test size in model function=\",test_data.shape[0])\n","  train_data=data_stat(train_data)\n","  print(\"train size after removing duplicates=\",train_data.shape[0],\"test size after removing duplicates=\",test_data.shape[0])\n","  \n","  X_train=train_data[:,1:train_data.shape[1]]\n","  Y_train=train_data[:,0]\n","  X_test=test_data[:,1:test_data.shape[1]]\n","  Y_test=test_data[:,0]\n","  \n","  \n","\n","  # define model\n","  model = RandomForestClassifier(n_estimators=50, random_state=42)\n","  \n","  s=model.fit(X_train,Y_train)\n","\n","  Y_pred=s.predict(X_test)\n","  Y_predTrain=s.predict(X_train)\n","  #importance = s.feature_importances_\n","  #print(\"Y_train=\",Y_train)\n","  \n","  print(\"Y_test=\",Y_test)\n","  print(\"Y_pred=\",Y_pred)\n","  \n","  \n","  # f1 score\n","  print(\"Train accuracy=\",f1_score(Y_train,Y_predTrain,average=\"macro\"))\n","  train_score=f1_score(Y_train,Y_predTrain,average=\"macro\")\n","  print(\"Test f1=\",f1_score(Y_test,Y_pred,average=\"macro\"))\n","  test_score=f1_score(Y_test,Y_pred,average=\"macro\")\n","  print(\"Test accuracy=\",accuracy_score(Y_test,Y_pred)) \n","  accu_score=accuracy_score(Y_test,Y_pred)\n","  print(classification_report(Y_test, Y_pred))\n","  \n","\n","  return train_score,test_score\n"]},{"cell_type":"markdown","source":["Create train and test set"],"metadata":{"id":"2i9VkNB65Cl3"}},{"cell_type":"code","execution_count":9,"metadata":{"id":"3-a7NQpGoRk4","executionInfo":{"status":"ok","timestamp":1644284082897,"user_tz":-330,"elapsed":407,"user":{"displayName":"Salma Mandi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjuIgiKzXUJnsrVeYqDh-c8HgFzaMf1rPA6e6_k8A=s64","userId":"16790298494727486665"}}},"outputs":[],"source":["def feature_file_creation(train_data,test_data,common):\n","  # column_list need to add as argument when using correlation\n","  feature_no=len(common)\n","  accu_score=0\n","  et_norm=np.empty((0))\n","  et_from_last_label=np.array([0,0])\n","  train_feature=np.empty((0,feature_no+4))\n","  test_feature=np.empty((0,feature_no+4))\n","  transition_matrix=find_transition_matrix(train_data)\n","  \n","  continuous_seq_len=1\n","  \n","  for i in range(0,train_data.shape[0]):\n","  \n","    pre,pre_2,label_wt=find_PRE(transition_matrix,train_data[i,0])\n","    # compute elapsed time\n","    if train_data[i,0]==0:\n","          et_from_last_label[0]=train_data[i,4]\n","          et_from_last_label[1]=et_from_last_label[1]+train_data[i,4]\n","    elif train_data[i,0]==1: \n","          et_from_last_label[1]=train_data[i,4] \n","          et_from_last_label[0]=et_from_last_label[0]+train_data[i,4]\n","        \n","    max_et=np.amax(et_from_last_label)\n","    \n","    if(max_et==0):\n","           continue\n","    et_norm=np.divide(et_from_last_label,float(max_et))\n","    # influence calculation\n","    for j in range(0,2):\n","          et_norm[j]=1-et_norm[j]\n","          label_wt[j]=label_wt[j]*et_norm[j]\n","    #print(\"influence:\",label_wt)      \n","    if train_data[i,0]==train_data[i,1]:\n","          continuous_seq_len=continuous_seq_len+1\n","    else:\n","          continuous_seq_len=1\n","\n","    img_features=facial_feature(train_data[i,3],common)  \n","    self_report=train_data[i,1]\n","    train_instance_lst=[]\n","    train_instance_lst.append(self_report)\n","    train_instance_lst.append(label_wt[0])\n","    train_instance_lst.append(label_wt[1])\n","    train_instance_lst.append(continuous_seq_len)\n","    train_instance_lst.extend(img_features)\n","    \n","    \n","    train_instance=np.array(train_instance_lst).reshape(1,len(train_instance_lst))\n","    \n","    # training data\n","    train_feature=np.append(train_feature,train_instance,axis=0)\n","    \n","  \n","  # for test feature\n","  continuous_seq_len=1\n","  old_label=test_data[0,0]\n","  for p in range(0,test_data.shape[0]):\n","    pre,pre_2,label_wt=find_PRE(transition_matrix,old_label) \n","    if old_label==0:\n","          et_from_last_label[0]=test_data[p,4]\n","          et_from_last_label[1]=et_from_last_label[1]+test_data[p,4]\n","    elif old_label==1:\n","          et_from_last_label[1]=test_data[p,4]\n","          et_from_last_label[0]=et_from_last_label[0]+test_data[p,4]\n","        \n","    max_et=np.amax(et_from_last_label)\n","    \n","    if(max_et==0):\n","\n","          continue\n","    et_norm=np.divide(et_from_last_label,float(max_et))\n","    for j in range(0,2):\n","          et_norm[j]=1-et_norm[j]\n","          label_wt[j]=label_wt[j]*et_norm[j]\n","    if old_label==test_data[p,1]:\n","          continuous_seq_len=continuous_seq_len+1\n","    else:\n","          continuous_seq_len=1\n","\n","    img_features=facial_feature(test_data[p,3],common)      \n","    self_report=test_data[p,1] \n","    test_instance_lst=[]\n","    # typing feature considered\n","    test_instance_lst.append(self_report)\n","    test_instance_lst.append(label_wt[0])\n","    test_instance_lst.append(label_wt[1])\n","    test_instance_lst.append(continuous_seq_len)\n","    test_instance_lst.extend(img_features)\n","    \n","    test_instance=np.array(test_instance_lst).reshape(1,len(test_instance_lst))\n","\n","    test_feature=np.append(test_feature,test_instance,axis=0)\n","    old_label=find_val(pre)\n","\n"," \n","  return train_feature,test_feature"]},{"cell_type":"markdown","source":["Excute the below code block to list the features obtain from Amazon Rekognition"],"metadata":{"id":"lcKEYCuv3ta_"}},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1127,"status":"ok","timestamp":1644284094308,"user":{"displayName":"Salma Mandi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjuIgiKzXUJnsrVeYqDh-c8HgFzaMf1rPA6e6_k8A=s64","userId":"16790298494727486665"},"user_tz":-330},"id":"cqT9pmWfRkXm","outputId":"89b53555-fc62-49a7-c462-df66d1d9b0c7"},"outputs":[{"output_type":"stream","name":"stdout","text":["(444, 65)\n","['eyeLeftX', 'eyeLeftY', 'eyeRightX', 'eyeRightY', 'mouthLeftX', 'mouthLeftY', 'mouthRightX', 'mouthRightY', 'noseX', 'noseY', 'leftEyeBrowLeftX', 'leftEyeBrowLeftY', 'leftEyeBrowRightX', 'leftEyeBrowRightY', 'leftEyeBrowUpX', 'leftEyeBrowUpY', 'rightEyeBrowLeftX', 'rightEyeBrowLeftY', 'rightEyeBrowRightX', 'rightEyeBrowRightY', 'rightEyeBrowupX', 'rightEyeBrowupY', 'leftEyeLeftX', 'leftEyeLeftY', 'leftEyeRightX', 'leftEyeRightY', 'leftEyeUpX', 'leftEyeUpY', 'leftEyeDownX', 'leftEyeDownY', 'rightEyeLeftX', 'rightEyeLeftY', 'rightEyeRightX', 'rightEyeRightY', 'rightEyeUpX', 'rightEyeUpY', 'rightEyeDownX', 'rightEyeDownY', 'noseLeftX', 'noseLeftY', 'noseRightX', 'noseRightY', 'mouthUpX', 'mouthUpY', 'mouthDownX', 'mouthDownY', 'leftPupilX', 'leftPupilY', 'rightPupilX', 'rightPupilY', 'upperJawlineLeftX', 'upperJawlineLeftY', 'midJawlineLeftX', 'midJawlineLeftY', 'chinBottomX', 'chinBottomY', 'midJawlineRightX', 'midJawlineRightY', 'upperJawlineRightX', 'upperJawlineRightY', 'PoseRoll', 'PoseYaw', 'PosePitch', 'QualityBrightness', 'QualitySharpness']\n"]}],"source":["# driver code\n","user_no=12\n","common=[]\n","user_name='user_'+str(format(user_no,'02'))\n","\n","dirname='Data/FacialData/Amazon/'+user_name+'.csv'\n","\n","array=pd.read_csv(dirname)\n","array=array.drop_duplicates(keep='last')\n","array=array.drop(['FileName',\"TapEmotion\",'NewTarget'],axis=1)\n","array=array.drop(['SmileValue_ False','SmileValue_ True','EyeglassesValue_ False','SunglassesValue_ False','SunglassesValue_ True',\"GenderValue_ 'Female'\",\"GenderValue_ 'Male'\",\"BeardValue_ False\",\n","                            \"BeardValue_ True\",\"MustacheValue_ False\",\"EyesOpenValue_ False\",\"EyesOpenValue_ True\",\"MouthOpenValue_ False\",\"MouthOpenValue_ True\",\"MustacheValue_ True\",'EyeglassesValue_ True'],axis=1,errors='ignore')  \n","array=array.drop(['Confidence','BoundingBoxWidth','BoundingBoxHeight','BoundingBoxLeft','BoundingBoxTop','AgeRangeLow','AgeRangeHigh','SmileConfidence', 'EyesOpenConfidence', 'MouthOpenConfidence','EyeglassesConfidence','SunglassesConfidence','GenderConfidence','BeardConfidence','MustacheConfidence','Date'], axis=1,errors='ignore')                          \n","column_list=list(array.columns)\n","print(array.shape)\n","common=column_list\n","print(common)"]},{"cell_type":"markdown","source":["Excute the code to list the features obtain from Google Vision"],"metadata":{"id":"qbaDCzBd4mdf"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":395,"status":"ok","timestamp":1641484571237,"user":{"displayName":"Salma Mandi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjuIgiKzXUJnsrVeYqDh-c8HgFzaMf1rPA6e6_k8A=s64","userId":"16790298494727486665"},"user_tz":-330},"id":"ln8qJPcFOuBE","outputId":"41c8c6f7-3162-45ae-e120-4e125ff89b15"},"outputs":[{"name":"stdout","output_type":"stream","text":["(86, 108)\n","['LEFT_EYEX', 'LEFT_EYEY', 'LEFT_EYEZ', 'RIGHT_EYEX', 'RIGHT_EYEY', 'RIGHT_EYEZ', 'RIGHT_EYEX.1', 'RIGHT_EYEY.1', 'RIGHT_EYEZ.1', 'LEFT_OF_LEFT_EYEBROWX', 'LEFT_OF_LEFT_EYEBROWY', 'LEFT_OF_LEFT_EYEBROWZ', 'RLX', 'RLY', 'RLZ', 'LRX', 'LRY', 'LRZ', 'RRX', 'RRY', 'RRZ', 'MBEX', 'MBEY', 'MBEZ', 'NTX', 'NTY', 'NTZ', 'ULX', 'ULY', 'ULZ', 'LLX', 'LLY', 'LLZ', 'MLX', 'MLY', 'MLZ', 'MRX', 'MRY', 'MRZ', 'MCX', 'MCY', 'MCZ', 'NBRX', 'NBRY', 'NBRZ', 'NBLX', 'NBLY', 'NBLZ', 'NBCX', 'NBCY', 'NBCZ', 'LETBX', 'LETBY', 'LETBZ', 'LERCX', 'LERCY', 'LERCZ', 'LEBBX', 'LEBBY', 'LEBBZ', 'LELCX', 'LELCY', 'LELCZ', 'RETBX', 'RETBY', 'RETBZ', 'RERCX', 'RERCY', 'RERCZ', 'REBBX', 'REBBY', 'REBBZ', 'RELCX', 'RELCY', 'RELCZ', 'LEUMX', 'LEUMY', 'LEUMZ', 'REUMX', 'REUMY', 'REUMZ', 'LETX', 'LETY', 'LETZ', 'RETX', 'RETY', 'RETZ', 'FGX', 'FGY', 'FGZ', 'CGX', 'CGY', 'CGZ', 'CLGX', 'CLGY', 'CLGZ', 'CRGX', 'CRGY', 'CRGZ', 'ThirtyFiveX', 'ThirtyFiveY', 'ThirtyFiveZ', 'ThirtySixX', 'ThirtySixY', 'ThirtySixZ', 'roll_angle', 'pan_angle', 'tilt_angle']\n"]}],"source":["# for vision data processing\n","# driver code\n","user_no=12\n","common=[]\n","user_name='user_'+str(format(user_no,'02'))\n","dirname='Data/FacialData/Google/'+user_name+'.csv'\n","\n","array=pd.read_csv(dirname)\n","array=array.drop_duplicates(keep='last')\n","array=array.drop(['FileName'],axis=1)\n","\n","array=array.drop(['bounding_poly1x', 'bounding_poly1y', 'bounding_poly2x', 'bounding_poly2y', 'bounding_poly3x', 'bounding_poly3y', 'bounding_poly4x', 'bounding_poly4y', 'fdbounding_poly1x', 'fdbounding_poly1y', 'fdbounding_poly2x', 'fdbounding_poly2y', 'fdbounding_poly3x', 'fdbounding_poly3y', 'fdbounding_poly4x', 'fdbounding_poly4y','detection_confidence','landmarking_confidence','joy_likelihood','sorrow_likelihood','anger_likelihood','surprise_likelihood','emotion','target','Date'],axis=1,errors='ignore')  \n","             \n","column_list=list(array.columns)\n","#print(column_list)\n","print(array.shape)\n","common=column_list\n","print(common)"]},{"cell_type":"markdown","source":["Excute the code to list features obtain from Microsoft Azure"],"metadata":{"id":"CuWxr4b443xU"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":378,"status":"ok","timestamp":1641493169347,"user":{"displayName":"Salma Mandi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjuIgiKzXUJnsrVeYqDh-c8HgFzaMf1rPA6e6_k8A=s64","userId":"16790298494727486665"},"user_tz":-330},"id":"q45rzQoEjlrc","outputId":"12bdbe37-49e6-484a-de17-080668233fae"},"outputs":[{"name":"stdout","output_type":"stream","text":["(90, 40)\n","['pupil_leftX', 'pupil_leftY', 'pupil_rightX', 'pupil_rightY', 'nose_tipX', 'nose_tipY', 'mouth_leftX', 'mouth_leftY', 'mouth_rightX', 'mouth_rightY', 'eyebrow_left_outerX', 'eyebrow_left_outerY', 'eyebrow_left_innerX', 'eyebrow_left_innerY', 'eye_left_outerX', 'eye_left_outerY', 'eye_left_topX', 'eye_left_topY', 'eye_left_bottomX', 'eye_left_bottomY', 'eye_left_innerX', 'eye_left_innerY', 'eyebrow_right_innerX', 'eyebrow_right_innerY', 'eyebrow_right_outerX', 'eyebrow_right_outerY', 'eye_right_innerX', 'eye_right_innerY', 'eye_right_topX', 'eye_right_topY', 'nose_right_alar_topX', 'nose_right_alar_topY', 'nose_left_alar_out_tipX', 'nose_left_alar_out_tipY', 'nose_right_alar_out_tipX', 'nose_right_alar_out_tipY', 'upper_lip_topX', 'upper_lip_topY', 'upper_lip_bottomX', 'upper_lip_bottomY']\n"]}],"source":["# for Azure Data Processing\n","user_no=12\n","common=[]\n","user_name='user_'+str(format(user_no,'02'))\n","dirname='Data/FacialData/Microsoft/'+user_name+'.csv'\n","array=pd.read_csv(dirname)\n","array=array.drop_duplicates(keep='last')\n","array=array.drop(['FileName'],axis=1)\n","column_list=list(array.columns)\n","print(array.shape)\n","common=column_list\n","print(common)"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9589,"status":"ok","timestamp":1644284193411,"user":{"displayName":"Salma Mandi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjuIgiKzXUJnsrVeYqDh-c8HgFzaMf1rPA6e6_k8A=s64","userId":"16790298494727486665"},"user_tz":-330},"id":"UQcwqi4HBP3e","outputId":"68e8380c-835f-4e3e-f1c3-165c5c0d9565"},"outputs":[{"output_type":"stream","name":"stdout","text":["data size= 146\n","fold no= 1\n","train data index= 0 --> 80 test data index= 81 ---> 110\n","train_data size= 81 test data size= 30\n","no. of high samples in train= 74 no of low samples in train= 7\n","train feature shape: (80, 69)\n","test feature shape: (30, 69)\n","(80, 4)\n","(30, 4)\n","train size in model function= 80 test size in model function= 30\n","number of unique rows= (80, 5)\n","train size after removing duplicates= 80 test size after removing duplicates= 30\n","Y_test= [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1.]\n","Y_pred= [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1.]\n","Train accuracy= 1.0\n","Test f1= 1.0\n","Test accuracy= 1.0\n","              precision    recall  f1-score   support\n","\n","         0.0       1.00      1.00      1.00         3\n","         1.0       1.00      1.00      1.00        27\n","\n","    accuracy                           1.00        30\n","   macro avg       1.00      1.00      1.00        30\n","weighted avg       1.00      1.00      1.00        30\n","\n","fold no= 2\n","train data index= 30 --> 110 test data index= 111 ---> 140\n","train_data size= 81 test data size= 30\n","no. of high samples in train= 75 no of low samples in train= 6\n","train feature shape: (81, 69)\n","test feature shape: (30, 69)\n","(81, 4)\n","(30, 4)\n","train size in model function= 81 test size in model function= 30\n","number of unique rows= (81, 5)\n","train size after removing duplicates= 81 test size after removing duplicates= 30\n","Y_test= [1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1.]\n","Y_pred= [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1.]\n","Train accuracy= 1.0\n","Test f1= 0.7115384615384615\n","Test accuracy= 0.8666666666666667\n","              precision    recall  f1-score   support\n","\n","         0.0       1.00      0.33      0.50         6\n","         1.0       0.86      1.00      0.92        24\n","\n","    accuracy                           0.87        30\n","   macro avg       0.93      0.67      0.71        30\n","weighted avg       0.89      0.87      0.84        30\n","\n","fold no= 3\n","train data index= 60 --> 140 test data index= 141 ---> 170\n","train_data size= 81 test data size= 5\n","no. of high samples in train= 69 no of low samples in train= 12\n","train feature shape: (79, 69)\n","test feature shape: (5, 69)\n","(79, 4)\n","(5, 4)\n","train size in model function= 79 test size in model function= 5\n","number of unique rows= (79, 5)\n","train size after removing duplicates= 79 test size after removing duplicates= 5\n","Y_test= [1. 1. 1. 1. 1.]\n","Y_pred= [1. 1. 1. 1. 1.]\n","Train accuracy= 1.0\n","Test f1= 1.0\n","Test accuracy= 1.0\n","              precision    recall  f1-score   support\n","\n","         1.0       1.00      1.00      1.00         5\n","\n","    accuracy                           1.00         5\n","   macro avg       1.00      1.00      1.00         5\n","weighted avg       1.00      1.00      1.00         5\n","\n","f1 score list= [1.0, 0.7115384615384615, 1.0]\n","Train f1-score= 1.0\n","Test f1-score= 0.9038461538461539\n"]}],"source":["# driver code\n","dirname='Data/Self_reports/'\n","\n","for user in [user_no]:  \n","  # read user file\n","  test_user='user_'+str(format(user,'02'))\n","  test_dir=dirname+test_user+'.csv'\n","  if(os.path.isfile(test_dir)):\n","\n","    dataset=pd.read_csv(test_dir)\n","    dataset=dataset.drop_duplicates(keep='last')\n","  else:\n","\n","    continue  \n","\n","  test_data=dataset.values\n","  data_size=test_data.shape[0]  \n","  print(\"data size=\",data_size)\n","  # specify training and testing data size for each iteration of cross-validation\n","  test_data_size=ceil(data_size*0.20)\n","  train_data_size=ceil(data_size*0.55)\n","  start=0\n","  train_data_index=0\n","  test_data_index=0\n","\n","  accu_list=[]\n","   \n","  train_accu=[] \n","  i=1\n","  \n","\n"," \n","  array1=np.array([])\n","\n","  # loop for the nested cross-validation \n","  while(start+train_data_size<test_data.shape[0]):\n","     print(\"fold no=\",i)\n","     train_data_index=start+train_data_size\n","     test_data_index=start+train_data_size+test_data_size\n","     print(\"train data index=\",start,\"-->\",train_data_index-1,\"test data index=\",train_data_index,\"--->\",test_data_index-1)\n","     print(\"train_data size=\",test_data[start:train_data_index,:].shape[0],\"test data size=\",test_data[train_data_index:test_data_index,:].shape[0])\n","     train_dataset=test_data[start:train_data_index,:]\n","     test_dataset=test_data[train_data_index:test_data_index,:]\n","    \n","     \n","    # calculate number of high and low samples in the train set\n","     if(test_dataset.shape[0]!=0):\n","       high_train=0\n","       low_train=0\n","       for k in range(train_dataset.shape[0]):\n","            if(train_dataset[k][1]==0):\n","               low_train+=1\n","            elif(train_dataset[k][1]==1):\n","               high_train+=1\n","       print(\"no. of high samples in train=\",high_train,\"no of low samples in train=\",low_train) \n","       if(low_train==0 or high_train==0):\n","         continue\n","       train_feature,test_feature=feature_file_creation(train_dataset,test_dataset,common)\n","       array1=train_dataset\n","       \n","      \n","       print(\"train feature shape:\",train_feature.shape)\n","       print(\"test feature shape:\",test_feature.shape)\n","       # separte targets from features\n","       X_train=train_feature[:,1:train_feature.shape[1]]   \n","       \n","       Y_train=train_feature[:,0]\n","       X_test=test_feature[:,1:test_feature.shape[1]]\n","    \n","       Y_test=test_feature[:,0]\n","       # seperate facial features\n","       new_train=X_train[:,3:X_train.shape[1]]\n","       new_test=X_test[:,3:X_test.shape[1]]\n","      \n","       # do standarization on facial data\n","       \n","       scaler = StandardScaler()\n","       #scaler.fit(X_train)\n","       scaler.fit(new_train)\n","       new_train=scaler.transform(new_train)\n","       new_test=scaler.transform(new_test)\n","       np.set_printoptions(suppress=True)\n","       \n","\n","      \n","       \n","       # reduce dimension of features\n","       \n","       # apply PCA \n","       pca=KernelPCA(n_components=1,kernel='rbf',eigen_solver='arpack',remove_zero_eig=True,random_state=32)\n","       \n","       pca.fit(new_train)\n","       new_train=pca.transform(new_train)\n","       new_test=pca.transform(new_test)\n","       \n","       \n","       \n","\n","       # apply KLDA\n","       # uncomment the below code when KLDA is applied\n","       '''\n","       klda=kda.KDA(n_components=1,kernel='rbf')\n","       new_train=klda.fit_transform(new_train,Y_train)\n","       new_test=klda.transform(new_test)\n","       '''\n","       \n","       # concatenate self-report features and facial features (obtain from feature reduction tool)\n","       trainX_new=np.concatenate((X_train[:,0:3],new_train),axis=1)\n","       testX_new=np.concatenate((X_test[:,0:3],new_test),axis=1)\n","      \n","       print(trainX_new.shape)\n","       print(testX_new.shape)\n","       Y_train=Y_train.reshape(len(Y_train),1)\n","       Y_test=Y_test.reshape(len(Y_test),1)\n","       \n","       \n","       final_train=np.concatenate((Y_train,trainX_new),axis=1)\n","       final_test=np.concatenate((Y_test,testX_new),axis=1)\n","       \n","       \n","       train_score,score=Random_Forest_model(final_train,final_test)  \n","       \n","      \n","       accu_list.append(score)\n","       \n","       train_accu.append(train_score)\n","\n","     start=start+test_data_size\n","     i+=1  \n","\n","  # print the accuracy     \n","  print(\"f1 score list=\",accu_list)\n","  if(len(train_accu)!=0):   \n","      print(\"Train f1-score=\",mean(train_accu))    \n","  if(len(accu_list)!=0):   \n","      print(\"Test f1-score=\",mean(accu_list))\n","\n","  \n","      \n"]}]}